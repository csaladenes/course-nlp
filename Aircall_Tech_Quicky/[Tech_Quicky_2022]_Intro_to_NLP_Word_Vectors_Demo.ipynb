{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Tech Quicky 2022] Intro_to_NLP_Word_Vectors_Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1x2ZZl6kLSzX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLd_H_4NIfgU"
      },
      "source": [
        "# plotting libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# store plot outputs within the notebook document\n",
        "%matplotlib inline\n",
        "\n",
        "# efficient numerical computation library\n",
        "import numpy as np\n",
        "\n",
        "# word and text similarity modeling library\n",
        "import gensim.downloader\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpRwUk5xJeeJ"
      },
      "source": [
        "# GloVe: Global Vectors for Word Representation\n",
        "\n",
        "Research paper written by Stanford researchers, came out in 2014.\n",
        "\n",
        "* unsupervised learning algorithm for obtaining vector representations \n",
        "* uses aggregated global word-word co-occurrence statistics\n",
        "* 50-300 dimensions to represent words and word meanings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exM9wVeFxE7V"
      },
      "source": [
        "# we are using the 100 dimensional vectors, which still give good results\n",
        "model = gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "print(type(model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(model.wv.vocab)"
      ],
      "metadata": {
        "id": "zydcazJSZHAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.index2word[20000:20010]"
      ],
      "metadata": {
        "id": "kRkvwVpOZkAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGYTWignxMjw"
      },
      "source": [
        "model['bread'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL-CawJKx5uw"
      },
      "source": [
        "model['bread']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarity"
      ],
      "metadata": {
        "id": "ogawyTAkdpAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('bread')"
      ],
      "metadata": {
        "id": "EoXo6Gbebk-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('haircut')"
      ],
      "metadata": {
        "id": "ziEqBUiJcApr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.similarity('bread','butter')"
      ],
      "metadata": {
        "id": "F0cOB4LqbJIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.similarity('bread','headache')"
      ],
      "metadata": {
        "id": "wwH7p9U6bLvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.similarity('river','ship')"
      ],
      "metadata": {
        "id": "RKmv7sbebPt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.similarity('river','banana')"
      ],
      "metadata": {
        "id": "CdmNYPkEbT0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R0z6vYIQoSY"
      },
      "source": [
        "# Arithmetic with words\n",
        "\n",
        "* word vectors showcases interesting linear substructures of the word vector space\n",
        "* we can notice linear structures in similarity and relatedness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq7GMWMkx_D4"
      },
      "source": [
        "# start with a capital city + madrid \n",
        "# substract the country component - spain\n",
        "# add another country component + france\n",
        "# result, get back the capital of the new country\n",
        "\n",
        "res = model.most_similar(positive=['madrid','france'], negative=['spain'])\n",
        "res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXXMmpUV0Dgv"
      },
      "source": [
        "def analogy_isto(X1, istoX2, Y1):\n",
        "  top10 = model.most_similar(positive=[istoX2,Y1], negative=[X1])\n",
        "  return [res for res in top10[:2]]\n",
        "analogy_isto('man','king','woman')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JzPFGX85Acm"
      },
      "source": [
        "analogy_isto('germany','beer','france') # start with drink, substract/add country"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analogy_isto('strong','stronger','weak')\n",
        "analogy_isto('sister','aunt','brother') # start with activity, substrac/add tool"
      ],
      "metadata": {
        "id": "nfvodiHweP14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogy_isto('brush','painting','camera') # start with activity, substrac/add tool\n",
        "analogy_isto('university','professor','school')"
      ],
      "metadata": {
        "id": "_tcun7egetOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk0POCM49kev"
      },
      "source": [
        "#analogy_isto('1','2','3')\n",
        "#analogy_isto('love','red','hate')\n",
        "analogy_isto('usa','obama','russia')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IAqm1beyXnx"
      },
      "source": [
        "analogy_isto('100','200','300')\n",
        "#analogy_isto('computer','mouse','screen')\n",
        "#analogy_isto('running','shoe','swimming')\n",
        "#analogy_isto('running','shoe','tennis')\n",
        "#analogy_isto('beer','wine','rum')\n",
        "#analogy_isto('usa','obama','hungary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-7MF0P8PxCV"
      },
      "source": [
        "# knift + death\n",
        "model.most_similar(positive=['knife','death'])\n",
        "#model.most_similar(positive=['hollywood','leonardo'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlXBofaJQAYI"
      },
      "source": [
        "# horse - farm\n",
        "model.most_similar(positive=['horse'], negative=['farm'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNLDPe2Iy1BC"
      },
      "source": [
        "model.most_similar(positive=['love'], negative=['red'])\n",
        "model.most_similar(positive=['house'], negative=['building'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NS2Wq6RCCYv"
      },
      "source": [
        "Can you find other cool/interesting relationships in the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting into 2D - finding clusters"
      ],
      "metadata": {
        "id": "haN-Us40dt8M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFPFGXRgA_AR"
      },
      "source": [
        "# ml library, used here for dimensionality reduction\n",
        "# Linear dimensionality reduction project data to a lower dimensional space\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def display_pca_scatterplot(model,words):\n",
        "  word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "  two_dim_vectors = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "  plt.figure(figsize=(6,6))\n",
        "  plt.scatter(two_dim_vectors[:,0], two_dim_vectors[:,1])\n",
        "  for w, (x,y) in zip(words, two_dim_vectors):\n",
        "    plt.annotate(w, (x,y), xytext=(x+0.05,y+0.05))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KecBfFO2CWVs"
      },
      "source": [
        "display_pca_scatterplot(model,['hungary','spain','romania', 'iceland', 'bear', 'snake', 'dolphin','computer','database','video',])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmwF7ZSwM3lv"
      },
      "source": [
        "Can you find sets of words that map in different regions, based on meanings?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# But why do these work?\n",
        "\n",
        "* This is not obvious, there is nothing in the optimization algorithm that should result in these linear substructures\n",
        "* There are some theories, but we donâ€™t know yet :) "
      ],
      "metadata": {
        "id": "TmQW_-UIcuue"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOKHcQrp72cg"
      },
      "source": [
        "# Key learnings\n",
        "\n",
        "\n",
        "* A surprising result - word meaning can be represented well by large vectors of numbers\n",
        "* These vectors can be calculated using a \"simple\" task of calculating and updating distributional similarities\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dibwak_0Mnnq"
      },
      "source": [
        "References\n",
        "\n",
        "- [Stanford Online: NLP with Deep Learning\n",
        "](https://www.youtube.com/watch?v=rmVRLeJRkl4&list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&index=1)\n",
        "- [Gensim - python library for easy use of word vectors and topic modeling](https://radimrehurek.com/gensim/auto_examples/index.html#documentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFMYKozgM1Oh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}